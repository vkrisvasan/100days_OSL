{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRMXaAXeHMP4N+ysmvN5Kh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkrisvasan/100days_OSL/blob/main/8_OSL__Crawl4AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%capture\n",
        "!pip install -U crawl4ai\n",
        "!pip install nest_asyncio\n",
        "!playwright install"
      ],
      "metadata": {
        "id": "k8Z6JyjsVy1U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "neH0NIhNV3rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "w3CUpBTHV5oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler, CacheMode\n",
        "\n",
        "async def simple_crawl():\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.buildfastwithai.com/\",\n",
        "            cach_mode = CacheMode.ENABLED # Default is ENABLED\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print the first 500 characters\n",
        "\n",
        "asyncio.run(simple_crawl())"
      ],
      "metadata": {
        "id": "pjnd0g7KV9Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def crawl_dynamic_content():\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        js_code = [\n",
        "            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n",
        "        ]\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            js_code=js_code,\n",
        "            # wait_for=wait_for,\n",
        "            cach_mode = CacheMode.ENABLED # Default is ENABLED\n",
        "        )\n",
        "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print first 500 characters\n",
        "\n",
        "asyncio.run(crawl_dynamic_content())"
      ],
      "metadata": {
        "id": "QGCaPLqqV_bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "async def clean_content():\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://en.wikipedia.org/wiki/Apple\",\n",
        "            excluded_tags=['nav', 'footer', 'aside'],\n",
        "            remove_overlay_elements=True,\n",
        "            # word_count_threshold=10,\n",
        "            cach_mode = CacheMode.ENABLED,\n",
        "            markdown_generator=DefaultMarkdownGenerator(\n",
        "                content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0),\n",
        "                options={\n",
        "                    \"ignore_links\": True\n",
        "                }\n",
        "            ),\n",
        "\n",
        "        )\n",
        "        full_markdown_length = len(result.markdown_v2.raw_markdown)\n",
        "        fit_markdown_length = len(result.markdown_v2.fit_markdown)\n",
        "        print(f\"Full Markdown Length: {full_markdown_length}\")\n",
        "        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n",
        "\n",
        "\n",
        "asyncio.run(clean_content())"
      ],
      "metadata": {
        "id": "21bU_stbWB59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def link_analysis():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            cach_mode = CacheMode.ENABLED,\n",
        "            exclude_external_links=True,\n",
        "            exclude_social_media_links=True,\n",
        "            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n",
        "        )\n",
        "        print(f\"Found {len(result.links['internal'])} internal links\")\n",
        "        print(f\"Found {len(result.links['external'])} external links\")\n",
        "\n",
        "        for link in result.links['internal'][:5]:\n",
        "            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n",
        "\n",
        "\n",
        "asyncio.run(link_analysis())"
      ],
      "metadata": {
        "id": "WQX47saeWD_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def media_handling():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://www.nbcnews.com/business\",\n",
        "            cach_mode = CacheMode.ENABLED,\n",
        "            exclude_external_images=False,\n",
        "            # screenshot=True # Set this to True if you want to take a screenshot\n",
        "        )\n",
        "        for img in result.media['images'][:5]:\n",
        "            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n",
        "\n",
        "asyncio.run(media_handling())"
      ],
      "metadata": {
        "id": "5WfmHiGUWEtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crawl4ai.extraction_strategy import LLMExtractionStrategy\n",
        "from pydantic import BaseModel, Field\n",
        "import os, json\n",
        "\n",
        "\n",
        "class OpenAIModelFee(BaseModel):\n",
        "    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n",
        "    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n",
        "    output_fee: str = Field(\n",
        "        ..., description=\"Fee for output token for the OpenAI model.\"\n",
        "    )\n",
        "\n",
        "async def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):\n",
        "    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n",
        "\n",
        "    # Skip if API token is missing (for providers that require it)\n",
        "    if api_token is None and provider != \"ollama\":\n",
        "        print(f\"API token is required for {provider}. Skipping this example.\")\n",
        "        return\n",
        "\n",
        "    extra_args = {\"extra_headers\": extra_headers} if extra_headers else {}\n",
        "\n",
        "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://openai.com/api/pricing/\",\n",
        "            word_count_threshold=1,\n",
        "            extraction_strategy=LLMExtractionStrategy(\n",
        "                provider=provider,\n",
        "                api_token=api_token,\n",
        "                schema=OpenAIModelFee.schema(),\n",
        "                extraction_type=\"schema\",\n",
        "                instruction=\"\"\"Extract all model names along with fees for input and output tokens.\"\n",
        "                \"{model_name: 'GPT-4', input_fee: 'US\n",
        "30.00 / 1M tokens'}.\"\"\",\n",
        "                **extra_args\n",
        "            ),\n",
        "            cach_mode = CacheMode.ENABLED\n",
        "        )\n",
        "        print(json.loads(result.extracted_content)[:5])\n",
        "\n",
        "# Usage:\n",
        "await extract_structured_data_using_llm(\"openai/gpt-4o-mini\", os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "7gPh3B8mWJwT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}